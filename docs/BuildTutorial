#   First clone locally the repository

git clone https://github.com/AlecioP/DBSCAN-distributed

#   Then move to the local repository

cd DBSCAN-distributed

#   In order to build a jar file to execute remotely on EMR cluster 
#   we use SBT package manager (A package manager for JAVA and SCALA like MAVEN)

#   To install sbt you must have JDK installed so run :

#   MACOS
#brew install openjdk

#   If you do not have homebrew installed
#/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

#   UBUNTU
#sudo apt-get install openjdk-11-jdk

#   Then you can install SBT : 

#   MACOS
#brew install sbt

#   UBUNTU 
#sudo apt-get install sbt

#   Read the build.sbt file to understand all about dependencies

#   Mainly we are adding the spark dependency with the specific version 
#   required from EMR cluster, but we are marking this dependency as "provided"
#   because the EMR cluster has the library already installed

#   Then we extract a thin jar, which means that we are not including in the 
#   archive any source file from the dependencies but only our app's source files

#   To compile 
sbt compile

#   To create the JAR file 
sbt package

#   Then we only need to run our application on a EMR cluster

#   To do that first go to your AWS console 
#   and open the EC2 service page https://console.aws.amazon.com/ec2/

#   Once there, from the menu on the left go to Network & Security > Key Pairs
#   Here you can create a pair of keys to for remote ssh-ing an ec2 machine
#   Follow the wizard, create the keys-pair, save you copy to your local machine
#   WARNING : Be aware of where the *.pem file has been saved on your machine
#   and ofcourse don't loose it 

#   Now we can create our cluster 
#   Go to EMR service page https://console.aws.amazon.com/elasticmapreduce/
#   Here, from the menu on the left click into "Clusters" then create a cluster
#   Select the number of nodes that compose your cluster, the kind of node according
#   to your needs, choose the Spark version to execute, but most importantly 
#   choose the key-pair you just created from the security menu

#   Now load into AWS the JAR we created previewsly 
#   To do that go to S3 service page https://s3.console.aws.amazon.com/s3/
#   Create a new bucket and load the JAR 

#   Now we have all ready to run our application
#   Go to your local machine and open a shell
#   Into the shell set a variable with the path to the aforementioned *.pem file

export AWS_AUTH_KEY="path/to/key/something.pem"

#   Than we need the url of the cluster. To obtain that, go to EMR service page
#   select the cluster from the list, click into cluster details and from the 
#   "Master public DNS" voice copy the URL provided. Now from your local shell

export AWS_MASTER_URL="Paste_here_the_url_you_just_copied"

#   At this point we can access the cluster driver via ssh

ssh -i $AWS_AUTH_KEY $AWS_MASTER_URL

#   Finally from the remote EC2 instance shell we can run our application via

spark-submit s3://URL_OF_JAR_INTO_S3 --data-file URL_OF_DATASET

