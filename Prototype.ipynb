{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "weighted-indian",
   "metadata": {},
   "source": [
    "# Algoritmo  DBSCAN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-shore",
   "metadata": {},
   "source": [
    "<p>\n",
    "L'algoritmo DBSCAN e' un algoritmo di clustering,<br> \n",
    "che divide un dataset in un numero di gruppi non <br>\n",
    "fissato a priori.\n",
    "\n",
    "Questo algoritmo itera il punti del dataset :<br>\n",
    "**foreach** *p* **in** *DS*\n",
    "\n",
    "per ogni *p* determina tutti i punti nella sua <br>\n",
    "neighborhood calcolando la funzione <br>\n",
    "*distance(p1:(Float&plus;),p2:(Float&plus;)):Boolean*.<br>\n",
    "\n",
    "Una volta calcolati tutti i punti li conta, se questo <br>\n",
    "conteggio *c* risulta essere minore di un certo valore <br>\n",
    "prestabilito *minCount* allora *p* e' etichettato con <br>\n",
    "label *NOISE*, altrimenti diventa il primo punto di un <br>\n",
    "nuovo cluster.<br>\n",
    "    \n",
    "A questo punto ogni punto nella neighborhood di *p* entra <br> \n",
    "a far parte del cluster se a sua volta nella sua <br>\n",
    "neighborhood ci sono almeno *minCount* punti.<br>\n",
    "\n",
    "Ricorsivamente ogni punto avvia la stessa computazione <br>\n",
    "su ogni punto a lui vicino. <br>\n",
    "    \n",
    "L'algoritmo termina nel momento un cui tutti i punti <br>\n",
    "in *DS* sono stati etichettati.<br>\n",
    "    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-equipment",
   "metadata": {},
   "source": [
    "![title](img/visual.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-bones",
   "metadata": {},
   "source": [
    "## Implementazione"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-courtesy",
   "metadata": {},
   "source": [
    "Per prima cosa generiamo un dataset di prova\n",
    "(Codice preso dall'esempio k-means visto a lezione) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pursuant-peace",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://macbook-pro:4040\n",
       "SparkContext available as 'sc' (version = 3.0.2, master = local[*], app id = local-1614678417589)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/*\n",
    "import java.io._\n",
    "import scala.util.Random\n",
    "\n",
    "val fileName = \"clusterin\" \n",
    "\n",
    "val epsilon = 0.0001\n",
    "val numK = 4\n",
    "\n",
    "val randomX = new Random\n",
    "val randomY = new Random\n",
    "val maxCoordinate = 100.0\n",
    "\n",
    "def genFile() = {\n",
    "\n",
    "    val initPoints = Vector((50.0,50.0),(50.0,-50.0),(-50.0,50.0),(-50.0,-50.0))\n",
    "\n",
    "    val distance = 80.0\n",
    "    val numPoints = 1000\n",
    "\n",
    "    val randomPoint = new Random\n",
    "\n",
    "    val file = new File(fileName)\n",
    "    val bw = new BufferedWriter(new FileWriter(file))\n",
    "    for (i <- 0 until numPoints) {\n",
    "      val x = (randomX.nextDouble-0.5) * distance\n",
    "      val y = (randomX.nextDouble-0.5) * distance\n",
    "      val centroid = initPoints(randomPoint.nextInt(initPoints.length))\n",
    "      bw.write((centroid._1+x)+\"\\t\"+(centroid._2 + y)+\"\\n\")\n",
    "    }\n",
    "    bw.close\n",
    "}\n",
    "\n",
    "genFile()\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-surveillance",
   "metadata": {},
   "source": [
    "A questo punto abbiamo un dataset da importare nella <br>\n",
    "nostra applicazione, andando a costruire un RDD dalla <br>\n",
    "libreria Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "loving-toolbox",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linesList: org.apache.spark.rdd.RDD[String] = clusterin MapPartitionsRDD[1] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val linesList = sc.textFile(\"clusterin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-vulnerability",
   "metadata": {},
   "source": [
    "La funzione **SparkContext.textFile** restituisce<br>\n",
    "una struttura lineare dove ogni elemento e' <br>\n",
    "una riga del file in input<br><br>\n",
    "Convertiamo questa struttura in <br>\n",
    "una struttura che contenente delle <br>\n",
    "coppie di *Float* che rappresentano<br>\n",
    "le coordinate di punti nel piano cartesiano<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "challenging-margin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "regex: String = \\s+\n",
       "toFloat: (s: String)Float\n",
       "toCouple: (a: Array[Float])Array[(Float, Float)]\n",
       "points: org.apache.spark.rdd.RDD[(Float, Float)] = MapPartitionsRDD[2] at flatMap at <console>:38\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val regex = \"\\\\s+\"\n",
    "def toFloat(s: String): Float = {\n",
    "  try {\n",
    "    s.toFloat\n",
    "  } catch {\n",
    "    case e: Exception => 0\n",
    "  }\n",
    "}\n",
    "def toCouple(a : Array[Float]) : Array[(Float,Float)]= {\n",
    "val c =(a(0),a(1))\n",
    "Array(c)\n",
    "}\n",
    "val points=linesList.flatMap(x=>toCouple(x.split(regex).map(x=>toFloat(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-differential",
   "metadata": {},
   "source": [
    "Cominciamo col creare una mappa per etichettare<br>\n",
    "ogni punto nel dataset. Inizializziamo tutte le<br>\n",
    "etichetta al valore **UNDEF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "geographic-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "//val label= collection.mutable.Map(points.collect().map(x => (x, \"undef\")).toMap.toSeq:_*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-extra",
   "metadata": {},
   "source": [
    "A questo avviamo un iterazione su tutti i punti<br>\n",
    "nel dataset per assegnare un etichetta ad ognuno<br><br>\n",
    "\n",
    "Prima di avviare la computazione, dobbiamo definire<br>\n",
    "due valori costanti a priori, che sono **Epsilon** e **MinCount** <br><br>\n",
    "\n",
    "Inoltre dobbiamo definire la funzione di distanza<br>\n",
    "tra due punti dello spazio. Nel nostro caso questa<br>\n",
    "sara semplicemente la distanza euclidea tra i punti<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "specified-celebration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epsilon: Int = 4\n",
       "minCount: Int = 5\n",
       "distance: (p1: (Float, Float), p2: (Float, Float))Double\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val epsilon = 4\n",
    "val minCount = 5\n",
    "\n",
    "// SQRT[ (x1-x2)^2 + (y1-y2)^2 ]\n",
    "def distance(p1:(Float,Float),p2:(Float,Float))= \n",
    "    math.sqrt(math.pow(p1._1-p2._1,2)+math.pow(p1._2-p2._2,2))\n",
    "\n",
    "//for(p<-points) {[MAIN-LOOP]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "romantic-ticket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cmp: (p1: (Float, Float), p2: (Float, Float))Boolean\n",
       "bCount: (anRdd: org.apache.spark.rdd.RDD[(Float, Float)])Int\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*\n",
    "RDD.count() may be inefficient :\n",
    "\n",
    "https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala\n",
    "\n",
    "Better version of count with less shuffling\n",
    "*/\n",
    "\n",
    "//(index,(x,y))\n",
    "\n",
    "def cmp(p1:(Float,Float),p2:(Float,Float)): Boolean = {((p1._1 == p2._1) && (p1._2 == p2._2))}\n",
    "\n",
    "def bCount(anRdd : org.apache.spark.rdd.RDD[(Float,Float)]) = \n",
    "    anRdd.map(x => (\"same\", (1,x) )).reduceByKey( (v1,v2) => (v1._1+v2._1,v1._2) ).collect()(0)._2._1.toInt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alternative-machine",
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "\n",
    "+++ DRIVER +++ \n",
    "\n",
    "label = (p0,\"undef\")(p1,\"undef\")(p2,\"undef\")(p3,\"undef\")(p4,\"undef\")\n",
    "        (p5,\"undef\")(p6,\"undef\")(p7,\"undef\")(p8,\"undef\")(p9,\"undef\")\n",
    "\n",
    "points = p0,p1,p2,p3,p4,p5,p6,p7,p8,p9\n",
    "\n",
    "+++ EXECUTOR +++ \n",
    "\n",
    "        d(p0,p1) < epsilon ==> T ok        |\n",
    "p0      d(p0,p2) < epsilon ==> F           |\n",
    "        d(p0,p3) < epsilon ==> F           |\n",
    "                                           |\n",
    "        d(p0,p4) < epsilon ==> F           |\n",
    "p0      d(p0,p5) < epsilon ==> T ok  == >  | 4 > minCount ==> T then label(p) = Nuovo Cluster C1\n",
    "        d(p0,p6) < epsilon ==> T ok        |\n",
    "                                           |\n",
    "        d(p0,p7) < epsilon ==> F           |\n",
    "p0      d(p0,p8) < epsilon ==> F           |\n",
    "        d(p0,p9) < epsilon ==> T ok        |\n",
    "\n",
    "\n",
    "p0\n",
    "|\n",
    "Neigh\n",
    "|\n",
    "V\n",
    "p1 p5 p6 p9\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*/\n",
    "\n",
    "//val broadcast = sc.broadcast(points.collect())\n",
    "\n",
    "// TRANSIENT : Do Not serialize -> Make shadow clone and send to executor OR \n",
    "//                                 Every executor accesses the structure in the driver\n",
    "\n",
    "//val n = collection.mutable.Map(m.toSeq: _*) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sharp-agent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "driverP: Array[(Float, Float)] = Array((68.91163,70.236565), (-49.951214,75.41598), (56.683727,-81.50358), (78.19354,-48.998375), (59.338333,-16.076706), (10.631957,-22.62603), (23.734114,79.80073), (-66.13613,26.729124), (-82.04394,-38.204617), (58.344402,-66.100845), (-44.638706,-35.001926), (27.847843,-14.053459), (-81.129105,-63.317368), (56.90097,-42.932392), (13.518054,-67.89066), (-38.164722,-22.266972), (20.965889,79.52757), (47.960464,27.071844), (-62.08853,-18.139246), (-25.185183,60.083454), (30.995613,66.38445), (-76.5989,66.340126), (-25.958687,-79.96308), (33.769836,84.42081), (69.146545,-43.170654), (68.26491,-53.60493), (-48.639442,75.15114), (45.199493,39.25411), (77.375755,-83.406105), (-40.31639,25.264), (-38.129612,54.245655), (-17.222492,20.794317), (10.840457,37.23...\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val driverP = points.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "interstate-belfast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; for details, enable `:setting -feature' or `:replay -feature'\n",
       "UNDEF: Int = -2\n",
       "NOISE: Int = -1\n",
       "labels: scala.collection.mutable.Map[(Float, Float),Int] = Map((-26.737846,-29.028118) -> -2, (-46.422974,60.680676) -> -2, (-67.845314,-72.5589) -> -2, (71.79205,13.414671) -> -2, (-35.85161,-66.17435) -> -2, (-30.83262,32.457336) -> -2, (10.30113,30.395073) -> -2, (-71.66183,-51.553772) -> -2, (28.525827,-88.101524) -> -2, (-64.180916,66.85124) -> -2, (20.965889,79.52757) -> -2, (34.099545,52.428307) -> -2, (33.940353,49.822643) -> -2, (17.352041,31.939344) -> -2, (59.485733,-50.259525) -> -2, (-69.202515,-25.072294) -> -2, (-20.404226,51.43441) -> -2, (76.83104,30.883263) -> -2, (-67.84203,50.42258) -> -2, (-18.024965,17.504507) -> -2, (15.463147,-16.491982) -> -2, (73.77406,-87.554054) -> -2, (77.79294,-25.839052) -> -2, (61.246582,-51.704727) -> -2, ...\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val UNDEF : Int = -2\n",
    "val NOISE : Int = -1\n",
    "\n",
    "val labels = collection.mutable.Map(   points.collect().map( (_,UNDEF) )   toSeq : _*)\n",
    "//var (dPx,dPy) = driverP.unzip\n",
    "val dim = points.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "binding-tissue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clusterNum: Int = 0\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var clusterNum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "interstate-democracy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.util.control._\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.control._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "reserved-scale",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outbreak: scala.util.control.Breaks = scala.util.control.Breaks@6291618c\n",
       "inbreak: scala.util.control.Breaks = scala.util.control.Breaks@7d342b96\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@transient val outbreak = new Breaks;\n",
    "@transient val inbreak = new Breaks;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "checked-auckland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW CLUSTER 1\n",
      "NEW CLUSTER 2\n",
      "NEW CLUSTER 3\n",
      "NEW CLUSTER 4\n",
      "NEW CLUSTER 5\n",
      "NEW CLUSTER 6\n",
      "NEW CLUSTER 7\n",
      "NEW CLUSTER 8\n",
      "NEW CLUSTER 9\n",
      "NEW CLUSTER 10\n",
      "NEW CLUSTER 11\n",
      "NEW CLUSTER 12\n",
      "NEW CLUSTER 13\n",
      "NEW CLUSTER 14\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for(it <- 0 until dim.toInt){outbreak.breakable{\n",
    "    \n",
    "    if( labels(driverP(it))!= UNDEF ) {outbreak.break}//CONTINUE \n",
    "    \n",
    "    //EACH EXECUTOR HAS A SUBSET SbS OF ALL THE POINTS\n",
    "    //COMPUTE DISTANCE OF P FROM EACH POINT IN SbS\n",
    "    val p = sc.broadcast(driverP(it))\n",
    "    \n",
    "    //IN EXECUTOR\n",
    "    val neighs = points.filter(x => distance(p.value,x)<=epsilon)\n",
    "    \n",
    "    //COLLECT C IN DRIVER\n",
    "    \n",
    "    \n",
    "    var queue = neighs.collect().filter(!cmp(driverP(it),_)).toSet\n",
    "    val c = queue.size\n",
    "    //println(\"PRINT C \"+c.toString)\n",
    "    \n",
    "    if(c<minCount){\n",
    "        labels(driverP(it))=NOISE\n",
    "    }\n",
    "    else{\n",
    "        //CLUSTER LABEL \n",
    "        clusterNum = clusterNum + 1\n",
    "        labels(driverP(it))=clusterNum\n",
    "        \n",
    "        println(\"NEW CLUSTER \"+clusterNum.toString)\n",
    "        \n",
    "        \n",
    "        while(queue.size>0){inbreak.breakable{\n",
    "            val h =queue.head\n",
    "            val pStr = \"(\"+h._1.toString + \",\" +  h._2.toString + \")\"\n",
    "           \n",
    "            if(  labels(h)  == NOISE) {labels(h)= clusterNum}\n",
    "            if(  labels(h)  != UNDEF) {queue = queue.filter(!cmp(h,_));inbreak.break}\n",
    "            \n",
    "            //println(\"Add \"+pStr+\" to cluster \"+clusterNum.toString)\n",
    "            \n",
    "            labels(h) = clusterNum\n",
    "            \n",
    "            val q = sc.broadcast(h)\n",
    "            \n",
    "            val nN = points.filter(y => distance(q.value,y)<=epsilon)\n",
    "            \n",
    "            //The neighboors of the neighboors\n",
    "            val driverNn = nN.collect().toSet\n",
    "            \n",
    "            //val c1 = nN.count() ----------->MORE SHUFFLING BUT LESS COMPUTATION IN DRIVER\n",
    "            \n",
    "            val c1 = driverNn.size // ----->LESS SHUFFLING BUT MORE COMPUTATION IN DRIVER\n",
    "            //println(\"PRINT C1 \"+c1.toString)\n",
    "            if(c1>= minCount){\n",
    "                //println(\"Update queue for cluster \"+clusterNum.toString)\n",
    "                val nNeighs = driverNn.filter(labels(_)< (-1))\n",
    "                //REMOVE THE ELEMENT COMPUTED, ADD ITS NEIGHBORS\n",
    "                queue = queue.filter(!cmp(h,_) ) ++ nNeighs\n",
    "            }else{\n",
    "                //println(\"REMOVE \"+h.toString)\n",
    "                queue = queue.filter(!cmp(h,_) )\n",
    "            }\n",
    "            \n",
    "        }}\n",
    "    }\n",
    "    \n",
    "    //JOIN ALL EXECUTORS I GUESS\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "unable-policy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "boh1: scala.collection.mutable.Iterable[Int] = ArrayBuffer(-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1...\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var boh1 = labels map { case (k,v) => v } filter (_==(-1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sweet-firewall",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Int = 887\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boh1.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-stock",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
